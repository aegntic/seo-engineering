/**
 * Robots.txt Optimizer Strategy
 * 
 * Optimizes robots.txt files for better SEO:
 * - Creates robots.txt if missing
 * - Fixes incorrect directives
 * - Adds missing sitemap references
 * - Optimizes crawler access rules
 */

const path = require('path');
const siteAdapter = require('../siteAdapter');
const logger = require('../../utils/logger');

// Robots.txt best practices
const ROBOTS_RECOMMENDATIONS = {
  'user-agent': [
    { agent: '*', description: 'Applies to all crawlers' },
    { agent: 'Googlebot', description: 'Google\'s standard web crawler' },
    { agent: 'Googlebot-Image', description: 'Google\'s image crawler' },
    { agent: 'Bingbot', description: 'Microsoft Bing\'s crawler' }
  ],
  'directives': [
    { name: 'Allow', usage: 'Allow: /path/', description: 'Explicitly allow indexing' },
    { name: 'Disallow', usage: 'Disallow: /path/', description: 'Prevent indexing' },
    { name: 'Sitemap', usage: 'Sitemap: https://example.com/sitemap.xml', description: 'Location of sitemap' },
    { name: 'Crawl-delay', usage: 'Crawl-delay: 10', description: 'Seconds between crawler requests' }
  ],
  'common-patterns': [
    { pattern: '/wp-admin/', reason: 'Admin areas should be disallowed' },
    { pattern: '/wp-includes/', reason: 'WordPress system files should be disallowed' },
    { pattern: '/search', reason: 'Search results pages should be disallowed' },
    { pattern: '/tag/', reason: 'Tag pages are often duplicate content' },
    { pattern: '/cgi-bin/', reason: 'Server scripts should be disallowed' },
    { pattern: '/tmp/', reason: 'Temporary files should be disallowed' }
  ]
};

// Template for a basic robots.txt file
const BASIC_ROBOTS_TEMPLATE = `# robots.txt file for {{domain}}
# Generated by SEOAutomate

User-agent: *
Allow: /

# Disallow admin and system areas
Disallow: /wp-admin/
Disallow: /wp-includes/
Disallow: /cgi-bin/
Disallow: /tmp/

# Disallow duplicate content areas
Disallow: /search
Disallow: /*?s=
Disallow: /*?p=

# Sitemap location
Sitemap: {{sitemapUrl}}
`;

/**
 * Fixes robots.txt issues
 * @param {string} repoPath - Path to the repository
 * @param {string} filePath - Path to the file being fixed (robots.txt)
 * @param {Object} issue - Issue details
 * @param {Object} siteStructure - Site structure information
 * @param {Object} options - Configuration options
 * @returns {Promise<Object>} - Fix result
 */
async function fix(repoPath, filePath, issue, siteStructure, options = {}) {
  try {
    // Check if filePath is actually robots.txt
    if (!isRobotsTxt(filePath)) {
      return {
        success: false,
        error: 'Not a robots.txt file'
      };
    }
    
    // Keep track of changes
    const changes = [];
    
    // Apply appropriate fix based on issue subtype
    switch (issue.subType) {
      case 'missing-robots':
        return await createRobotsTxt(repoPath, filePath, issue.details);
      case 'missing-sitemap':
        changes.push(...await addMissingSitemap(repoPath, filePath, issue.details));
        break;
      case 'disallows-everything':
        changes.push(...await fixOverlyRestrictive(repoPath, filePath, issue.details));
        break;
      case 'incorrect-syntax':
        changes.push(...await fixSyntaxErrors(repoPath, filePath, issue.details));
        break;
      case 'optimize-directives':
        changes.push(...await optimizeDirectives(repoPath, filePath, issue.details));
        break;
      default:
        return {
          success: false,
          error: `Unknown robots.txt issue subtype: ${issue.subType}`
        };
    }
    
    // If no changes were made, return failure
    if (changes.length === 0) {
      return {
        success: false,
        error: 'No changes were needed or could be made'
      };
    }
    
    return {
      success: true,
      changes
    };
  } catch (error) {
    logger.error(`Robots.txt fix failed: ${error.message}`);
    return {
      success: false,
      error: `Failed to fix robots.txt: ${error.message}`
    };
  }
}

/**
 * Creates a new robots.txt file if missing
 * @param {string} repoPath - Path to the repository
 * @param {string} filePath - Path where robots.txt should be
 * @param {Object} details - Issue details
 * @returns {Promise<Object>} - Fix result
 */
async function createRobotsTxt(repoPath, filePath, details) {
  try {
    // Get domain and sitemap URL from details
    const domain = details.domain || 'example.com';
    const sitemapUrl = details.sitemapUrl || `https://${domain}/sitemap.xml`;
    
    // Prepare robots.txt content
    let content = BASIC_ROBOTS_TEMPLATE
      .replace(/{{domain}}/g, domain)
      .replace(/{{sitemapUrl}}/g, sitemapUrl);
    
    // Add additional CMS-specific directives if applicable
    if (details.cmsType) {
      content += getCmsSpecificRules(details.cmsType);
    }
    
    // Write the file
    await siteAdapter.writeFile(repoPath, filePath, content);
    
    return {
      success: true,
      changes: [{
        type: 'create',
        file: filePath,
        content: content.substring(0, 100) + '...' // Truncate for logging
      }]
    };
  } catch (error) {
    throw new Error(`Failed to create robots.txt: ${error.message}`);
  }
}

/**
 * Adds missing sitemap directive to robots.txt
 * @param {string} repoPath - Path to the repository
 * @param {string} filePath - Path to robots.txt
 * @param {Object} details - Issue details
 * @returns {Promise<Array>} - List of changes made
 */
async function addMissingSitemap(repoPath, filePath, details) {
  const changes = [];
  
  try {
    // Read the current robots.txt content
    const content = await siteAdapter.readFile(repoPath, filePath);
    
    // Check if a sitemap directive already exists
    const hasSitemap = /^Sitemap:/mi.test(content);
    
    if (!hasSitemap) {
      // Get sitemap URL from details
      const domain = details.domain || extractDomain(details.siteUrl || '');
      const sitemapUrl = details.sitemapUrl || `https://${domain}/sitemap.xml`;
      
      // Add sitemap directive at the end
      const updatedContent = content + `\n\n# Sitemap location\nSitemap: ${sitemapUrl}\n`;
      
      // Write the updated file
      await siteAdapter.writeFile(repoPath, filePath, updatedContent);
      
      changes.push({
        type: 'update',
        element: 'Sitemap directive',
        value: sitemapUrl
      });
    }
  } catch (error) {
    logger.error(`Failed to add sitemap to robots.txt: ${error.message}`);
  }
  
  return changes;
}

/**
 * Fixes robots.txt that disallows everything
 * @param {string} repoPath - Path to the repository
 * @param {string} filePath - Path to robots.txt
 * @param {Object} details - Issue details
 * @returns {Promise<Array>} - List of changes made
 */
async function fixOverlyRestrictive(repoPath, filePath, details) {
  const changes = [];
  
  try {
    // Read the current robots.txt content
    const content = await siteAdapter.readFile(repoPath, filePath);
    
    // Parse the robots.txt into a structured format
    const parsed = parseRobotsTxt(content);
    
    // Check for overly restrictive rules
    let overlyRestrictive = false;
    let updatedContent = content;
    
    for (const group of parsed.groups) {
      // Check if this group disallows everything
      if (group.directives.some(d => d.directive === 'Disallow' && d.value === '/')) {
        overlyRestrictive = true;
        
        // Replace "Disallow: /" with more specific disallows
        updatedContent = updatedContent.replace(
          new RegExp(`(User-agent:\\s*${escapeRegExp(group.userAgent)}[\\s\\S]*?)Disallow:\\s*/`, 'i'),
          (match, prefix) => {
            return `${prefix}Allow: /\n\n# Disallow admin and system areas\nDisallow: /wp-admin/\nDisallow: /cgi-bin/\nDisallow: /tmp/\n\n# Disallow duplicate content areas\nDisallow: /search\nDisallow: /*?s=`;
          }
        );
      }
    }
    
    if (overlyRestrictive) {
      // Write the updated file
      await siteAdapter.writeFile(repoPath, filePath, updatedContent);
      
      changes.push({
        type: 'update',
        element: 'Disallow directive',
        description: 'Replaced overly restrictive "Disallow: /" with more specific rules'
      });
    }
  } catch (error) {
    logger.error(`Failed to fix overly restrictive robots.txt: ${error.message}`);
  }
  
  return changes;
}

/**
 * Fixes syntax errors in robots.txt
 * @param {string} repoPath - Path to the repository
 * @param {string} filePath - Path to robots.txt
 * @param {Object} details - Issue details
 * @returns {Promise<Array>} - List of changes made
 */
async function fixSyntaxErrors(repoPath, filePath, details) {
  const changes = [];
  
  try {
    // Read the current robots.txt content
    let content = await siteAdapter.readFile(repoPath, filePath);
    
    // Fix common syntax errors
    
    // 1. Fix missing colon after directive
    const directiveNoColonRegex = /^(User-agent|Allow|Disallow|Sitemap|Crawl-delay)\s+([^:].*)$/gmi;
    content = content.replace(directiveNoColonRegex, '$1: $2');
    
    // 2. Fix incorrect spacing
    const incorrectSpacingRegex = /^(User-agent|Allow|Disallow|Sitemap|Crawl-delay):\s*([^\s].*)$/gmi;
    content = content.replace(incorrectSpacingRegex, '$1: $2');
    
    // 3. Fix directives not starting at beginning of line (with whitespace)
    const indentedDirectiveRegex = /^\s+(User-agent|Allow|Disallow|Sitemap|Crawl-delay):/gmi;
    content = content.replace(indentedDirectiveRegex, '$1:');
    
    // 4. Ensure User-agent comes before directives
    if (!/^User-agent:/mi.test(content) && /^(Allow|Disallow):/mi.test(content)) {
      content = `User-agent: *\n${content}`;
    }
    
    // 5. Fix invalid UTF-8 encoding
    content = decodeURIComponent(encodeURIComponent(content).replace(/%[0-9A-F]{2}/g, match => {
      try {
        return decodeURIComponent(match);
      } catch (e) {
        return '';
      }
    }));
    
    // Write the updated file
    await siteAdapter.writeFile(repoPath, filePath, content);
    
    changes.push({
      type: 'update',
      element: 'robots.txt syntax',
      description: 'Fixed syntax errors in robots.txt'
    });
  } catch (error) {
    logger.error(`Failed to fix robots.txt syntax: ${error.message}`);
  }
  
  return changes;
}

/**
 * Optimizes directives in robots.txt
 * @param {string} repoPath - Path to the repository
 * @param {string} filePath - Path to robots.txt
 * @param {Object} details - Issue details
 * @returns {Promise<Array>} - List of changes made
 */
async function optimizeDirectives(repoPath, filePath, details) {
  const changes = [];
  
  try {
    // Read the current robots.txt content
    let content = await siteAdapter.readFile(repoPath, filePath);
    
    // Parse the robots.txt
    const parsed = parseRobotsTxt(content);
    
    // Apply optimizations
    let updated = false;
    
    // Organize by user agent
    const organizedGroups = {};
    
    // Group directives by user agent
    for (const group of parsed.groups) {
      if (!organizedGroups[group.userAgent]) {
        organizedGroups[group.userAgent] = {
          allow: [],
          disallow: [],
          other: []
        };
      }
      
      for (const directive of group.directives) {
        if (directive.directive.toLowerCase() === 'allow') {
          organizedGroups[group.userAgent].allow.push(directive);
        } else if (directive.directive.toLowerCase() === 'disallow') {
          organizedGroups[group.userAgent].disallow.push(directive);
        } else {
          organizedGroups[group.userAgent].other.push(directive);
        }
      }
    }
    
    // Create optimized content
    let optimizedContent = '';
    
    // Add comments at the top if they exist
    const topComments = content.match(/^#.*$/gm);
    if (topComments) {
      optimizedContent += topComments.join('\n') + '\n\n';
    } else {
      optimizedContent += `# robots.txt for ${details.domain || 'this site'}\n`;
      optimizedContent += `# Optimized by SEOAutomate on ${new Date().toISOString().split('T')[0]}\n\n`;
    }
    
    // Add each user agent group
    for (const userAgent in organizedGroups) {
      optimizedContent += `User-agent: ${userAgent}\n`;
      
      // Add allows first (good practice)
      for (const allow of organizedGroups[userAgent].allow) {
        optimizedContent += `${allow.directive}: ${allow.value}\n`;
      }
      
      // Then disallows
      if (organizedGroups[userAgent].disallow.length > 0) {
        // Add a blank line if we added allows
        if (organizedGroups[userAgent].allow.length > 0) {
          optimizedContent += '\n';
        }
        
        for (const disallow of organizedGroups[userAgent].disallow) {
          optimizedContent += `${disallow.directive}: ${disallow.value}\n`;
        }
      }
      
      // Then other directives
      if (organizedGroups[userAgent].other.length > 0) {
        optimizedContent += '\n';
        for (const other of organizedGroups[userAgent].other) {
          optimizedContent += `${other.directive}: ${other.value}\n`;
        }
      }
      
      // Add a blank line between user agent groups
      optimizedContent += '\n';
    }
    
    // Add sitemap at the end if it exists
    if (parsed.sitemaps.length > 0) {
      optimizedContent += '# Sitemap\n';
      for (const sitemap of parsed.sitemaps) {
        optimizedContent += `Sitemap: ${sitemap}\n`;
      }
    } else if (details.sitemapUrl) {
      // Add sitemap if it was provided but missing
      optimizedContent += '# Sitemap\n';
      optimizedContent += `Sitemap: ${details.sitemapUrl}\n`;
      changes.push({
        type: 'add',
        element: 'Sitemap directive',
        value: details.sitemapUrl
      });
    }
    
    // Check if content is actually different
    if (content.trim() !== optimizedContent.trim()) {
      // Write the updated file
      await siteAdapter.writeFile(repoPath, filePath, optimizedContent);
      
      changes.push({
        type: 'optimize',
        element: 'robots.txt structure',
        description: 'Reorganized and optimized robots.txt directives'
      });
    }
  } catch (error) {
    logger.error(`Failed to optimize robots.txt directives: ${error.message}`);
  }
  
  return changes;
}

/**
 * Parses a robots.txt file into a structured format
 * @param {string} content - robots.txt content
 * @returns {Object} - Parsed structure
 */
function parseRobotsTxt(content) {
  const result = {
    groups: [],
    sitemaps: [],
    comments: []
  };
  
  let currentUserAgent = null;
  let currentGroup = null;
  
  // Split by lines and process each
  const lines = content.split('\n');
  
  for (const line of lines) {
    const trimmedLine = line.trim();
    
    // Skip empty lines
    if (!trimmedLine) continue;
    
    // Process comments
    if (trimmedLine.startsWith('#')) {
      result.comments.push(trimmedLine);
      continue;
    }
    
    // Process directives
    const colonIndex = trimmedLine.indexOf(':');
    if (colonIndex > 0) {
      const directive = trimmedLine.substring(0, colonIndex).trim();
      const value = trimmedLine.substring(colonIndex + 1).trim();
      
      // Check directive type
      const lowerDirective = directive.toLowerCase();
      
      if (lowerDirective === 'user-agent') {
        // Start new group
        currentUserAgent = value;
        currentGroup = {
          userAgent: currentUserAgent,
          directives: []
        };
        result.groups.push(currentGroup);
      } else if (lowerDirective === 'sitemap') {
        // Add sitemap URL
        result.sitemaps.push(value);
      } else if (currentGroup) {
        // Add to current group
        currentGroup.directives.push({
          directive: directive,
          value: value
        });
      }
    }
  }
  
  return result;
}

/**
 * Gets CMS-specific robots.txt rules
 * @param {string} cmsType - Type of CMS
 * @returns {string} - CMS-specific rules
 */
function getCmsSpecificRules(cmsType) {
  switch (cmsType.toLowerCase()) {
    case 'wordpress':
      return `
# WordPress specific rules
Disallow: /wp-admin/
Disallow: /wp-includes/
Disallow: /wp-content/plugins/
Disallow: /wp-json/
Disallow: /wp-content/cache/
Disallow: */trackback/
Disallow: */feed/
`;
    
    case 'shopify':
      return `
# Shopify specific rules
Disallow: /admin
Disallow: /cart
Disallow: /orders
Disallow: /checkout
Disallow: /34186076/orders
Disallow: /34186076/checkouts
Disallow: /*design_theme_id*
Disallow: /*preview_theme_id*
Disallow: /*preview_script_id*
`;
    
    case 'magento':
      return `
# Magento specific rules
Disallow: /app/
Disallow: /downloader/
Disallow: /lib/
Disallow: /media/downloadable/
Disallow: /pkginfo/
Disallow: /report/
Disallow: /var/
Disallow: /catalog/product_compare/
Disallow: /catalog/category/view/
Disallow: /catalog/product/view/
Disallow: /catalogsearch/
Disallow: /checkout/
Disallow: /control/
Disallow: /customer/
Disallow: /install/
Disallow: /newsletter/
`;
    
    default:
      return '';
  }
}

/**
 * Checks if a file is robots.txt
 * @param {string} filePath - Path to check
 * @returns {boolean} - True if it's robots.txt
 */
function isRobotsTxt(filePath) {
  const basename = path.basename(filePath).toLowerCase();
  return basename === 'robots.txt';
}

/**
 * Extracts domain from a URL
 * @param {string} url - URL to extract from
 * @returns {string} - Domain
 */
function extractDomain(url) {
  try {
    const urlObj = new URL(url);
    return urlObj.hostname;
  } catch (e) {
    // If URL is invalid, try to extract domain directly
    const match = url.match(/^(?:https?:\/\/)?([^\/]+)/i);
    return match ? match[1] : 'example.com';
  }
}

/**
 * Escapes special characters in a string for use in a regular expression
 * @param {string} string - String to escape
 * @returns {string} - Escaped string
 */
function escapeRegExp(string) {
  return string.replace(/[.*+?^${}()|[\]\\]/g, '\\$&');
}

module.exports = {
  fix,
  // Export internal functions for testing
  createRobotsTxt,
  addMissingSitemap,
  fixOverlyRestrictive,
  fixSyntaxErrors,
  optimizeDirectives,
  parseRobotsTxt
};